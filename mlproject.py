# -*- coding: utf-8 -*-
"""MLProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OWRfqkp5cUZNy0I8kUPdCbWv9m1_nzmn
"""

'''
Avinash Vadivelu - axv200086
Virtue Adowei - vea190000
Emmanuel Samuel - exs210034
Abhay Sood - axs190179
'''

'''
CS 4375.001 Semester Project

Implementing a recurrent neural network from scratch and using it for time series stock market data
'''

#import libraries
import numpy as np  #used for numpy array processing
import pandas as pd #used for pandas dataframe
import matplotlib.pyplot as plt #used for graphs and charts
from sklearn.model_selection import train_test_split  #used for preprocessing
from sklearn.preprocessing import MinMaxScaler        #used for preprocessing
from sklearn.metrics import r2_score                  #used for model evaluation
from sklearn.metrics import mean_squared_error        #used for model evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

df = pd.read_csv("https://raw.githubusercontent.com/blckcbe/ML_Project/master/vix-daily_csv.csv") #read in the dataframe

#plot the data distribution of the dataset
plt.hist(df['VIX Close'])
plt.xlabel('VIX CLOSE Values')
plt.ylabel('# of Instances')
plt.show()

#preprocess the data
df.dropna()                                   #drop null values
df.drop_duplicates()                          #drop duplicate values
X = df.drop(['VIX Close', 'Date'], axis=1)    #remove the target and the date for normalization
y = df['VIX Close']                           #extract the target
X['year'] = pd.DatetimeIndex(df['Date']).year #convert the string date to a numerical format: extract the year
X['month'] = pd.DatetimeIndex(df['Date']).month #convert the string date to a numerical format: extract the month
X['day'] = pd.DatetimeIndex(df['Date']).day   #convert the string date to a numerical format: extract the date
m = MinMaxScaler()                            #minmaxscaler to normalize the data
X = pd.DataFrame((m.fit(X)).transform(X))      #normalize the data
X_train, X_test, Y_train, Y_test = train_test_split(X, y) #generate train and test splits

#class to represent the RNN
class RNN():

  #initialize the class acording to the input size and hidden size hyperparameters
  def __init__(self, num_features, hidden_size, target_size = 1, num_layers = 3, learning_rate = 0.00001):
    #weights of the hidden layers
    self.Wx = np.random.randn(hidden_size, num_features)/1000
    self.Wh = np.random.randn(hidden_size, hidden_size)/1000
    self.Wy = np.random.randn(target_size, hidden_size)/1000
    #biases
    self.b = np.zeros((hidden_size, 1))
    self.by = np.zeros((target_size, 1))
    #hyperparameter for the number of hidden layers
    self.num_layers = num_layers
    #holds the states of the hidden layer at different time steps, serving as a "memory" component that allows for temporal dependencies
    self.hidden_layers = [np.zeros((hidden_size, 1))]
    #learning rate for backpropagation
    self.learning_rate = learning_rate


  #perform the forward pass on one set of inputs
  def forward(self, x):
        x = np.array(x).reshape(-1, 1) #reshape the input for later multiplication
        self.hidden_layers = [np.zeros((self.Wh.shape[0], 1))]  #initialize the hidden layers

        #iterate over all the hidden layers
        for i in range(self.num_layers):
            h_prev = self.hidden_layers[-1] #the previous hidden layer (produces inputs to the next hidden layer)
            h_next = np.dot(self.Wx, x) + np.dot(self.Wh, h_prev) + self.b #the next hidden layer is a combination of the outputs of previous layers
            h_next = 1 / (1 + np.exp(-1 * h_next)) #sigmoid activation function

            self.hidden_layers.append(h_next) #add the newest layer to the list of hidden layers

        last_hidden_layer = self.hidden_layers[-1]  #the last hidden layer links to the output layer
        output = np.dot(self.Wy, last_hidden_layer) + self.by #compute the output
        return output.flatten()[0]  #return the prediction of the rnn


  #perform the backward pass
  def backpropagation(self, input, output_error):
        #reshape inputs for multiplication
        input = np.array(input).reshape(-1, 1)
        #compute the partial derivative of for the output layer weights
        d_output_weights = output_error * self.hidden_layers[-1] * (1 - self.hidden_layers[-1])
        temp_delta = np.dot(d_output_weights, self.Wy)
        d_hidden_weights = np.zeros_like(self.Wh)

        #iterate over all the layers to compute the partial derivative for the hidden layer weights
        for i in reversed(range(1, self.num_layers + 1)):
            h = self.hidden_layers[i]
            d_hidden_weights += temp_delta * (1 - h) * h
            temp_delta += np.dot(d_hidden_weights, self.Wh)

        #compute the partial derivative of for the input layer weights
        d_input_weights = temp_delta @ ((1 - np.dot(self.Wx, input)) * np.dot(self.Wx, input))

      # clip the derivatives to avoid the vanishing gradient problem
        np.clip(d_input_weights, -1, 1, out=d_input_weights)
        np.clip(d_hidden_weights, -1, 1, out=d_hidden_weights)
        np.clip(d_output_weights, -1, 1, out=d_output_weights)

        #update weights and biases according to the gradient descent of the loss
        self.Wy = self.Wy - (self.learning_rate * np.dot(d_output_weights, self.hidden_layers[self.num_layers-1].T))
        self.Wh = self.Wh - (self.learning_rate * np.dot(d_hidden_weights, self.Wh.T))
        self.Wx = self.Wx - (self.learning_rate * np.dot(d_input_weights, input.T))
        self.by = self.by - (self.learning_rate * output_error)
        self.b = self.b - (self.learning_rate * temp_delta)

        return


#train the model on the dataset
  def train(self, features, target, epochs):
    history = {'RMSE': [], 'R2': []}
    #iterate according to the specified number of epochs
    for epoch in range(epochs):
      total_loss = 0.0
      predicted = []
      #iterate over all the instances in the dataset
      for i in range(len(features)):
        x = features.iloc[i]      #the features used to predict
        y_actual = target.iloc[i] #the actual value of the target variable

        # Perform the forward pass
        output = self.forward(x)
        predicted.append(output)

        # Apply backprogation
        self.backpropagation(x, output - y_actual)

      #model evaluation metrics
      rmse = (np.sqrt(mean_squared_error(target, predicted)))  #compute the root mean squared error
      r2 = r2_score(target, predicted)                         #compute the r2 score

      history['RMSE'].append(rmse)
      history['R2'].append(r2)

      print(f'Epoch {epoch + 1}, RMSE: {rmse}, R2 score: {r2}')  #print the result of each epoch

    return history


  def evaluate(self, test_x, test_y):
    predicted = []
    for i in range(test_x.shape[0]):
      # Perform the forward pass
      output = self.forward(test_x.iloc[i])
      predicted.append(output)
    #model evaluation metrics
    rmse = (np.sqrt(mean_squared_error(test_y, predicted)))  #compute the root mean squared error
    r2 = r2_score(test_y, predicted)                         #compute the r2 score

    print(f'RMSE: {rmse}, R2 score: {r2}')  #print the model evaluation metrics
    return predicted

# Initialize RNN
hidden_size = 50  # Example hidden size, adjust based on your needs
input_size = X_train.shape[1]  # This should be the number of features in your input
rnn_layer = RNN(input_size, hidden_size)

history = rnn_layer.train(X_train, Y_train, 15)

rnn_layer.evaluate(X_test,Y_test)

plt.figure(figsize=(10, 7))
plt.plot(history['RMSE'])
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.title('Epoch vs. RMSE')
plt.show()

# Plotting final predictions using the trained model

# predictions = []
# for i in range((X_test.shape[0])):
#     x_test_instance = X_test.iloc[i]
#     predictions.append(rnn_layer.forward(x_test_instance))
predictions = rnn_layer.evaluate(X_test,Y_test)

plt.figure(figsize=(30, 10))
plt.plot(predictions, label='Predicted VIX Close Prices', color='red')
#plt.plot(Y_test.values, label='Actual VIX Close Prices')
plt.xlabel('Intances')
plt.ylabel('Close Prices')
plt.title('Predicted VIX Close Prices')
plt.legend()
plt.show()

plt.figure(figsize=(30, 10))
plt.plot(Y_test.values, label='Actual VIX Close Prices')
plt.xlabel('Date')
plt.ylabel('VIX Close Prices')
plt.title('Actual VIX Close Prices')
plt.legend()
plt.show()

#Trials that can be logged
#trial 1
#trial1 = RNN(X_train.shape[1], hidden_size=40, target_size = 1, num_layers = 3, learning_rate = 0.0001)

#trial5 = RNN(X_train.shape[1], hidden_size=50, target_size = 1, num_layers = 3, learning_rate = 0.00001)

#trial6 = RNN(X_train.shape[1], hidden_size=60, target_size = 1, num_layers = 2, learning_rate = 0.00001)

#trial7 = RNN(X_train.shape[1], hidden_size=30, target_size = 1, num_layers = 2, learning_rate = 0.00001)

#trial8 = RNN(X_train.shape[1], hidden_size=20, target_size = 1, num_layers = 2, learning_rate = 0.00001)

#trial9 = RNN(X_train.shape[1], hidden_size=50, target_size = 1, num_layers = 2, learning_rate = 0.000001)

#trial10 = RNN(X_train.shape[1], hidden_size=40, target_size = 1, num_layers = 3, learning_rate = 0.0001)
# train = trial1.train(X_train, Y_train, num_epochs)
# eval = trial1.evaluate(X_test,Y_test)
#trial1 = RNN(X_train.shape[1], hidden_size)
#train = trial9.train(X_train, Y_train, 70)
#eval = trial9.evaluate(X_test,Y_test)